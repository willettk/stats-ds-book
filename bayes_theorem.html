

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta property="og:title" content="Bayes’ Theorem" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://cranmer.github.io/stats-ds-book/bayes_theorem.html" />
  <meta property="og:description" content="Earlier we discussed conditional probability for an event A given another event B: P(A \mid B). Examples: the probability to have N neutrons in an atom given an atomic number of Z plot, the distrib..." />
  <meta property="og:image" content="https://cranmer.github.io/stats-ds-book/_images/Neyman-pearson.006.png" />
  
    <title>Bayes’ Theorem &#8212; Statistics and Data Science</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx_tabs/semantic-ui-2.4.1/segment.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx_tabs/semantic-ui-2.4.1/menu.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx_tabs/semantic-ui-2.4.1/tab.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx_tabs/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/pdf_print.css" />
    <link rel="stylesheet" type="text/css" href="_static/spanels-index--0afa1ebdab0bb23b98b56d34c23d9f57.css" />
    <link rel="stylesheet" type="text/css" href="_static/spanels-variables--ffc7f74dcb1b9eecba2dbcbc22818714.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/save_state.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"N": "\\mathbb{N}", "indep": "{\\perp\\kern-5pt\\perp}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "bered": ["\\color{#DC2830}{#1}", 1], "ecol": ["}}"]}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Independence" href="independence.html" />
    <link rel="prev" title="Conditonal Probability" href="conditional.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Statistics and Data Science</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Statistics and Data Science
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  About the course
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="schedule.html">
   Draft Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jupyterhub.html">
   JupyterHub for class
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="discussion_forum.html">
   Discussion Forum
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preliminaries.html">
   Preliminaries
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Probability
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="reference internal" href="probability-topics.html">
   Probability Topics
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="random_variables.html">
     Random Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="conditional.html">
     Conditonal Probability
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Bayes’ Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="independence.html">
     Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="empirical_distribution.html">
     Empirical Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="expectation.html">
     Expectation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="correlation.html">
     Covariance and Correlation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="datasaurus-long.html">
     Simple data exploration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="distributions/visualize_marginals.html">
     Visualizing joint and marginal distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="measures_of_dependence.html">
     Quantifying statistical dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="distributions/change-of-variables.html">
     How do distributions transform under a change of variables ?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="distributions/one-over-x-flow.html">
     Change of variables with autodiff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="distributions/likelihood-change-obs.html">
     Transformation of likelihood with change of random variable
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="distributions/invariance-of-likelihood-to-reparameterizaton.html">
     Transformation properties of the likelihood and posterior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="error-propagation/investigating-propagation-of-errors.html">
     Investigating propagation of errors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="error-propagation/error_propagation_with_jax.html">
     Revisiting error propagation with automatic differentiation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="distributions/accept-reject.html">
     Accept / Reject Monte Carlo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="distributions/Binomial_histograms-interactive.html">
     An interactive exploration of statistical fluctuations in histograms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="pgm/daft.html">
     Visualizing Graphical Models
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="statistics-topics.html">
   Statistics Topics
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="neyman_pearson.html">
     Neyman-Pearson lemma
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neyman_construction.html">
     Neyman construction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lhc_stats_thumbnail.html">
     Thumbnail of LHC Statistical Procedures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="statistical_decision_theory.html">
     Statistical decision theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="probprog/MarkovPath.html">
     Universal Probabilistic Programming Example
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="prml_notebooks/attribution.html">
   PRML Examples
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch01_Introduction.html">
     1. Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch02_Probability_Distributions.html">
     2. Probability Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch03_Linear_Models_for_Regression.html">
     3. Linear Models for Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch04_Linear_Models_for_Classfication.html">
     4. Linear Models for Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch05_Neural_Networks.html">
     5. Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch06_Kernel_Methods.html">
     6. Kernel Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch07_Sparse_Kernel_Machines.html">
     7. Sparse Kernel Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch08_Graphical_Models.html">
     8. Graphical Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch09_Mixture_Models_and_EM.html">
     9. Mixture Models and EM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch10_Approximate_Inference.html">
     10. Approximate Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch11_Sampling_Methods.html">
     11. Sampling Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch12_Continuous_Latent_Variables.html">
     12. Continuous Latent Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prml_notebooks/ch13_Sequential_Data.html">
     13. Sequential Data
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Software and Computing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="computing-topics.html">
   Software &amp; Computing Topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="autodiff-tutorial.html">
   Tutorial on Automatic Differentiation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Data Science
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="data-science-topics.html">
   Data Science Topics
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="other_resources.html">
   Other Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="built-on.html">
   Built on
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Jupyter Book Reference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="markdown.html">
   Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cheatsheet.html">
   MyST Cheat Sheet
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks.html">
   Content with notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interactive.html">
   Interactive data visualizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="test_embed_video.html">
   Test Embed Video
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="nbgrader.html">
   nbgrader
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/bayes_theorem.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/cranmer/stats-ds-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/cranmer/stats-ds-book/issues/new?title=Issue%20on%20page%20%2Fbayes_theorem.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/cranmer/stats-ds-book/edit/master/book/bayes_theorem.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayes-rule-in-pictures">
   Bayes’ rule in pictures
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#breaking-down-the-terms">
   Breaking down the terms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#an-example">
   An example:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#axioms-of-probability">
   Axioms of probability
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bayes-theorem">
<h1>Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Permalink to this headline">¶</a></h1>
<p>Earlier we discussed <a class="reference internal" href="conditional.html"><span class="doc std std-doc"><strong>conditional probability</strong></span></a> for an event <span class="math notranslate nohighlight">\(A\)</span> <strong>given</strong> another event <span class="math notranslate nohighlight">\(B\)</span>: <span class="math notranslate nohighlight">\(P(A \mid B)\)</span>.
Examples:</p>
<ul class="simple">
<li><p>the probability to have <span class="math notranslate nohighlight">\(N\)</span> neutrons in an atom given an atomic number of <span class="math notranslate nohighlight">\(Z\)</span> <a class="reference external" href="https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Isotopes_and_half-life.svg/1280px-Isotopes_and_half-life.svg.png">plot</a></p></li>
<li><p>the distribution of height <span class="math notranslate nohighlight">\(h\)</span> given that you are a professional basketball player</p></li>
<li><p>the disribution of some generic data <span class="math notranslate nohighlight">\(X\)</span> given a theory with parameters <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p>the probability of testing negative for COVID19 given that you actually have COVID19</p></li>
</ul>
<p>Bayes’ rule allows us to invert the relationship from <span class="math notranslate nohighlight">\(P(A \mid B)\)</span> to <span class="math notranslate nohighlight">\(P(B \mid A)\)</span>.
It can also be thought of as updating our <strong>prior probability</strong> for <span class="math notranslate nohighlight">\(B\)</span> to a <strong>posterior probability</strong> for <span class="math notranslate nohighlight">\(B\)</span> given that we observe <span class="math notranslate nohighlight">\(A\)</span>.</p>
<div class="admonition-theorem-bayes-rule admonition">
<p class="admonition-title">Theorem (Bayes’ rule)</p>
<p>For any events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> in a probability space <span class="math notranslate nohighlight">\((\Omega,\mathcal{F},P)\)</span></p>
<div class="math notranslate nohighlight">
\[
P(B \mid A) = \frac{P (A \mid B)P (B)}{P(A)}
\]</div>
<p>as long as <span class="math notranslate nohighlight">\(P (A) &gt; 0\)</span>.</p>
</div>
<p>In our examples this would turn into:</p>
<ul class="simple">
<li><p>the probability for an atom to have an atomic number of <span class="math notranslate nohighlight">\(Z\)</span> given that it has <span class="math notranslate nohighlight">\(N\)</span> neutrons</p></li>
<li><p>the probability to be a professional basketball player given your height is <span class="math notranslate nohighlight">\(h\)</span></p></li>
<li><p>the probability disribution for a theory’s parameters <span class="math notranslate nohighlight">\(\theta\)</span> given data <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>the probability of actually having COVID19 given that you tested negative for COVID19</p></li>
</ul>
<div class="section" id="bayes-rule-in-pictures">
<h2>Bayes’ rule in pictures<a class="headerlink" href="#bayes-rule-in-pictures" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="id1">
<img alt="_images/Bayes-theorem-in-pictures.png" src="_images/Bayes-theorem-in-pictures.png" />
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">These images are adapted from lectures by Bob Cousins.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="breaking-down-the-terms">
<h2>Breaking down the terms<a class="headerlink" href="#breaking-down-the-terms" title="Permalink to this headline">¶</a></h2>
<p>Each of the terms in Bayes’ rule has a name and interpretation. For this I think it is useful to think not of generic <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, but to think of some theory of the Universe with parameters <span class="math notranslate nohighlight">\(\theta\)</span> (like the Higgs mass or the cosmological constant) and the predictions for what the data <span class="math notranslate nohighlight">\(X\)</span> would look like given <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(X \mid \theta)\)</span>: the <strong>likelihood</strong>: the probability distributon of the data <span class="math notranslate nohighlight">\(X\)</span> given the theoretical parameters <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p(\theta)\)</span>: the <strong>prior probability</strong> for the parameter <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p(\theta \mid X)\)</span>: the <strong>posterior probability</strong> of <span class="math notranslate nohighlight">\(\theta\)</span> given <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p(X)\)</span>: the normalizing constant often referred to as the <strong>evidence</strong>.</p></li>
</ul>
</div>
<div class="section" id="an-example">
<h2>An example:<a class="headerlink" href="#an-example" title="Permalink to this headline">¶</a></h2>
<p>To be concrete, consider this <a class="reference external" href="https://indico.cern.ch/event/197461/">plot from the ATLAS experiment at the Large Hadron Collider</a> from July 2012. It shows the distribution of a random variable <span class="math notranslate nohighlight">\(m_{4l}\)</span>) given three different hypothesized Higgs boson masses <span class="math notranslate nohighlight">\(m_H=(125, 150, 190)\)</span> GeV. You can think of the data as <span class="math notranslate nohighlight">\(\{m_{4l}\}=X\)</span> and the parameter as <span class="math notranslate nohighlight">\(m_H=\theta\)</span>.</p>
<div class="figure align-default" id="id2">
<a class="reference internal image-reference" href="_images/atlas-higgs-2012.png"><img alt="_images/atlas-higgs-2012.png" src="_images/atlas-higgs-2012.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">A <a class="reference external" href="https://indico.cern.ch/event/197461/">plot from the ATLAS experiment at the Large Hadron Collider</a> from July 2012. It shows histograms for the observed data (black dots) as well as the expected distribution for a random variable denoted <span class="math notranslate nohighlight">\(m_{4l}\)</span> given different hypothesized Higgs boson basses <span class="math notranslate nohighlight">\(m_H\)</span> (blue, orange, grey, which are stacked on top of the common <span class="math notranslate nohighlight">\(m_H\)</span>-independent backgrounds red+purple).</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>If we ask ourselves, what is the probability distribution for the Higgs mass given the data <span class="math notranslate nohighlight">\(p(m_H \mid \{ m_{4l}\} )\)</span> Bayes theorem tells us we need the likelihood <span class="math notranslate nohighlight">\(p(\{m_{4l}\} \mid m_H)\)</span>, which we can calculate using Quantum Field Theory <em>and</em> the prior probability <span class="math notranslate nohighlight">\(p(m_H)\)</span>. But where does <span class="math notranslate nohighlight">\(p(m_H)\)</span> come from? We cannot calculate that from Quantum Field Theory, it is simply a parameter of the theory. If we were to say that our prior <span class="math notranslate nohighlight">\(p(m_H)\)</span> is informed by some other experimental evidence <span class="math notranslate nohighlight">\(Y\)</span>, and it is really a posterior <span class="math notranslate nohighlight">\(p(m_H \mid Y)\)</span>, we would just find ourselves in the same situation for that previous measurement. Eventually we will be led to some original prior on <span class="math notranslate nohighlight">\(m_H\)</span>, which is not supported by experimental evidence or theoretical argument. More over, if we <em>define</em> probability  in as the frequency that an event occurs in a large number of trials, what is the ensemble of trials? These would correspond to different universes. That interpretation may be ok if you embrace the idea of the Multiverse (in fact, this is at the heart of the <a class="reference external" href="https://en.wikipedia.org/wiki/Anthropic_principle">anthropic principle</a>), but if you imagine a single universe with an unknown true value for <span class="math notranslate nohighlight">\(m_H\)</span>, then <span class="math notranslate nohighlight">\(p(m_H)\)</span> is simply not defined and it makes no sense to talk about a prior or a posterior on the parameter.</p>
</div>
<div class="section" id="axioms-of-probability">
<span id="axioms-of-prob"></span><h2>Axioms of probability<a class="headerlink" href="#axioms-of-probability" title="Permalink to this headline">¶</a></h2>
<p>It may be surprising to first learn  that there is not a unique definition of probability given how mathematical and formal probability and statistics are. There are two main “schools” usually refered to as Frequentist and Bayesian statistics. Frequentists do not deny that Bayes’ theorem is true – it’s a thoerem after all – but they do define probability in terms of the limit of long term frequency of an event occuring in multiple trials and, therefore, deny assigning probabilities to some quantities. Eg. the Higgs boson mass <span class="math notranslate nohighlight">\(m_H\)</span> is not a random variable, but simply a parameter that indexes (or parameterizes) of a family of distributions. In contrast, Bayesians tend to promote these parameters to random variables with corresponding probability distributions. How is this probability defined? There are many potential <a class="reference external" href="https://plato.stanford.edu/archives/sum2003/entries/probability-interpret/#1">interpretations of probability</a>, but a common interpretation for Bayesian statistics is a <strong>subjective degree of belief</strong>. It may seem surprising that one could use a subjective degree of belief in such a mathematical topic, but the formal mathematics of probability and statistics is sound as long as the probability function (or measure) <span class="math notranslate nohighlight">\(P\)</span> in the probability space <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span> satisfies <a class="reference external" href="https://en.wikipedia.org/wiki/Probability_axioms">Kolmogorov’s axioms of probability</a> (see also <a class="reference external" href="https://plato.stanford.edu/archives/sum2003/entries/probability-interpret/#1">Stanford Encyclopedia of Philosophy</a>). We saw these axioms when we first introduced <a class="reference internal" href="random_variables.html"><span class="doc std std-doc">random variables</span></a>.</p>
<p>The frequentist definition of probability in terms of limiting frequency of events across many trials satisfies Kolmogorov’s axioms (see criticism <a class="reference external" href="http://plato.stanford.edu/archives/sum2003/entries/probability-interpret/#3.1">here</a>). But <strong>how do you quantify subjective degree of belief</strong>? There is a nice article in the <a class="reference external" href="https://plato.stanford.edu/archives/sum2003/entries/probability-interpret/#3.5">Stanford Encyclopedia of Philosophy</a>, which I will quote from:</p>
<p>Subjective probabilities are traditionally analyzed in terms of betting behavior. Here is a classic statement by de Finetti (1980):</p>
<blockquote>
<div><p>Let us suppose that an individual is obliged to evaluate the rate p at which he would be ready to exchange the possession of an arbitrary sum <span class="math notranslate nohighlight">\(S\)</span> (positive or negative) dependent on the occurrence of a given event <span class="math notranslate nohighlight">\(E\)</span>, for the possession of the sum <span class="math notranslate nohighlight">\(pS\)</span>; we will say by definition that this number <span class="math notranslate nohighlight">\(p\)</span> is the measure of the degree of probability attributed by the individual considered to the event <span class="math notranslate nohighlight">\(E\)</span>, or, more simply, that <span class="math notranslate nohighlight">\(p\)</span> is the probability of <span class="math notranslate nohighlight">\(E\)</span> (according to the individual considered; this specification can be implicit if there is no ambiguity).</p>
</div></blockquote>
<p>This boils down to the following analysis:</p>
<blockquote>
<div><p>Your degree of belief in <span class="math notranslate nohighlight">\(E\)</span> is <span class="math notranslate nohighlight">\(p\)</span> iff <span class="math notranslate nohighlight">\(p\)</span> units of utility is the price at which you would buy or sell a bet that pays 1 unit of utility if <span class="math notranslate nohighlight">\(E\)</span>, 0 if not <span class="math notranslate nohighlight">\(E\)</span>.</p>
</div></blockquote>
<p>A <strong>Dutch book</strong> (against an agent) is a series of bets, each acceptable to the agent, but which collectively guarantee her loss, however the world turns out. Ramsey notes, and it can be easily proven (e.g., Skyrms 1984), that if your subjective probabilities violate the probability calculus, then you are susceptible to a Dutch book. For example, suppose that you violate the additivity axiom by assigning <span class="math notranslate nohighlight">\(P(A \cup  B) &lt; P(A) + P(B)\)</span>, where A and B are mutually exclusive. Then a cunning bettor could buy from you a bet on <span class="math notranslate nohighlight">\(A \cup B\)</span> for <span class="math notranslate nohighlight">\(P(A \cup B)\)</span> units, and sell you bets on <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> individually for <span class="math notranslate nohighlight">\(P(A)\)</span> and <span class="math notranslate nohighlight">\(P(B)\)</span> units respectively. He pockets an initial profit of <span class="math notranslate nohighlight">\(P(A) + P(B) - P(A \cup  B)\)</span>, and retains it whatever happens. Ramsey offers the following influential gloss: “If anyone’s mental condition violated these laws [of the probability calculus], his choice would depend on the precise form in which the options were offered him, which would be absurd.” (1980, 41)</p>
<p>Equally important, and often neglected, is the converse theorem that establishes how you can avoid such a predicament. If your subjective probabilities conform to the probability calculus, then no Dutch book can be made against you (Kemeny 1955); your probability assignments are then said to be <strong>coherent</strong>. In a nutshell, conformity to the probability calculus is necessary and sufficient for coherence.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="conditional.html" title="previous page">Conditonal Probability</a>
    <a class='right-next' id="next-link" href="independence.html" title="next page">Independence</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Kyle Cranmer<br/>
        
            &copy; Copyright .<br/>
          <div class="extra_footer">
            <div>
<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png"></a>
    All content on this site (unless otherwise specified) is licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0 license</a>
</div>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>